import streamlit as st
import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy.stats import pearsonr, t
import itertools
import io
import concurrent.futures
import os
import time
import multiprocessing

# ==========================================
# 0. UI ç¾åŒ–å·¥å…·
# ==========================================

def styled_tag(text, icon=""):
    st.markdown(f"""
    <div style="
        display: inline-flex;
        align-items: center;
        background-color: #e3f2fd;
        color: #1565c0;
        padding: 6px 16px;
        border-radius: 20px;
        font-weight: 600;
        font-size: 15px;
        margin-bottom: 15px;
        margin-top: 5px;
        border: 1px solid #bbdefb;
        box-shadow: 0 1px 2px rgba(0,0,0,0.05);
    ">
        <span style="margin-right: 8px; font-size: 18px;">{icon}</span>
        {text}
    </div>
    """, unsafe_allow_html=True)

# ==========================================
# 1. æ ¸å¿ƒç»Ÿè®¡å·¥å…· (ä¿æŒä¸å˜)
# ==========================================

def get_stars(p_value):
    if p_value < 0.001: return '***'
    if p_value < 0.01:  return '**'
    if p_value < 0.05:  return '*'
    return 'ns'

def pairwise_lsd_test_with_mse(stats_df, mse, df_resid, alpha=0.05):
    results = []
    group_names = stats_df.index.tolist()
    for g1, g2 in itertools.combinations(group_names, 2):
        m1, n1 = stats_df.loc[g1, 'mean'], stats_df.loc[g1, 'count']
        m2, n2 = stats_df.loc[g2, 'mean'], stats_df.loc[g2, 'count']
        diff = m1 - m2
        se = np.sqrt(mse * (1/n1 + 1/n2))
        if se <= 1e-10: 
            p_val = 1.0
        else:
            t_stat = abs(diff) / se
            p_val = 2 * (1 - t.cdf(t_stat, df_resid))
        reject = p_val < alpha
        results.append([g1, g2, diff, p_val, reject])
    return results

def solve_clique_cld(means, pairwise_data, use_uppercase=False):
    groups = [str(g).strip() for g in means.index.tolist()]
    n = len(groups)
    g_to_i = {g: i for i, g in enumerate(groups)}
    adj = np.ones((n, n), dtype=bool) 
    if pairwise_data:
        for row in pairwise_data:
            g1, g2, reject = str(row[0]).strip(), str(row[1]).strip(), row[4]
            if reject: 
                if g1 in g_to_i and g2 in g_to_i:
                    i, j = g_to_i[g1], g_to_i[g2]
                    adj[i, j] = False
                    adj[j, i] = False
    np.fill_diagonal(adj, False)
    cliques = []
    def bron_kerbosch(R, P, X):
        if len(P) == 0 and len(X) == 0:
            cliques.append(R)
            return
        union_px = P.union(X)
        if not union_px: pivot = None
        else: pivot = next(iter(union_px))
        neighbors_pivot = {idx for idx in range(n) if adj[pivot, idx]} if pivot is not None else set()
        for v in list(P - neighbors_pivot):
            neighbors_v = {idx for idx in range(n) if adj[v, idx]}
            bron_kerbosch(R.union({v}), P.intersection(neighbors_v), X.intersection(neighbors_v))
            P.remove(v)
            X.add(v)
    bron_kerbosch(set(), set(range(n)), set())
    clique_means = []
    for clq in cliques:
        avg_mean = np.mean([means.iloc[i] for i in clq])
        clique_means.append((avg_mean, clq))
    clique_means.sort(key=lambda x: x[0], reverse=True)
    
    letters_list = "ABCDEFGHIJKLMNOPQRSTUVWXYZ" if use_uppercase else "abcdefghijklmnopqrstuvwxyz"
    group_letters = {i: "" for i in range(n)}
    for idx, (avg, clq) in enumerate(clique_means):
        char = letters_list[idx] if idx < len(letters_list) else "?"
        for node_idx in clq:
            group_letters[node_idx] += char
    final_res = {}
    original_index = means.index.tolist()
    for i in range(n):
        l_str = "".join(sorted(group_letters[i]))
        final_res[str(original_index[i]).strip()] = l_str
    return final_res

# ==========================================
# 2. Worker å‡½æ•° (å¿…é¡»å®šä¹‰åœ¨é¡¶å±‚ï¼Œæ–¹ä¾¿å¤šè¿›ç¨‹è°ƒç”¨)
# ==========================================

def process_single_target(target, df_data, factors, test_factor, mse_strategy):
    """
    å•ä¸ªæŒ‡æ ‡çš„è®¡ç®—é€»è¾‘ï¼Œå°†è¢«å¤šè¿›ç¨‹è°ƒç”¨ã€‚
    """
    res = {
        'anova_rows': [],
        'main_effects_rows': [],
        'sliced_comparison_rows': [],
        'error': None
    }
    
    try:
        # æé€Ÿé¢„æ£€æŸ¥ï¼šå»ç©ºå€¼
        current_df = df_data.dropna(subset=[target] + factors).copy()
        
        if current_df.empty or len(current_df) < 3:
            return res 

        group_factors = [f for f in factors if f != test_factor]

        # 1. å…¨å±€ ANOVA
        factor_terms = [f'Q("{f}")' for f in factors]
        formula_rhs = " * ".join(factor_terms)
        formula = f"Q('{target}') ~ {formula_rhs}"
        
        model = ols(formula, data=current_df).fit()
        
        global_mse = model.mse_resid
        global_df_resid = model.df_resid
        
        aov_table = sm.stats.anova_lm(model, typ=2)
        aov_table.index = [idx.replace('Q("', '').replace('")', '') for idx in aov_table.index]

        for source, row in aov_table.iterrows():
            if source == 'Residual': continue
            f_str = f"{row['F']:.2f}{get_stars(row['PR(>F)'])}"
            res['anova_rows'].append({
                'Trait': target,
                'Source': source,
                'F_Sig': f_str
            })
        
        # 2. ä¸»æ•ˆåº”
        for factor in factors:
            stats = current_df.groupby(factor)[target].agg(['mean', 'std', 'count']).fillna(0)
            
            if mse_strategy == 'oneway':
                try:
                    sub_formula = f"Q('{target}') ~ C(Q('{factor}'))"
                    sub_model = ols(sub_formula, data=current_df).fit()
                    current_mse = sub_model.mse_resid
                    current_df_resid = sub_model.df_resid
                except:
                    current_mse = global_mse
                    current_df_resid = global_df_resid
            else:
                current_mse = global_mse
                current_df_resid = global_df_resid

            if len(stats) < 2:
                letters = {str(k).strip(): 'A' for k in stats.index}
            else:
                pairwise_res = pairwise_lsd_test_with_mse(stats, current_mse, current_df_resid, alpha=0.05)
                letters = solve_clique_cld(stats['mean'], pairwise_res, use_uppercase=True)
            
            for lvl in stats.index:
                lvl_str = str(lvl).strip()
                mean_val = stats.loc[lvl, 'mean']
                res['main_effects_rows'].append({
                    'Factor': factor,
                    'Level': lvl_str,
                    'Trait': target,
                    'Mean_Letter': f"{mean_val:.2f} {letters.get(lvl_str, 'A')}", 
                    'SD': stats.loc[lvl, 'std']
                })

        # 3. ç»„å†…æ¯”è¾ƒ
        if not group_factors:
            iter_groups = [( "All", current_df )] 
        else:
            iter_groups = current_df.groupby(group_factors)

        for group_keys, sub_df in iter_groups:
            if not isinstance(group_keys, tuple): group_keys = (group_keys,)
            
            current_info = {'Trait': target}
            if group_factors:
                for k, val in zip(group_factors, group_keys):
                    current_info[k] = str(val)
            
            stats = sub_df.groupby(test_factor)[target].agg(['mean', 'std', 'count']).fillna(0)
            
            if len(stats) < 2:
                letters = {str(k).strip(): 'a' for k in stats.index}
            else:
                try:
                    local_formula = f"Q('{target}') ~ C(Q('{test_factor}'))"
                    local_model = ols(local_formula, data=sub_df).fit()
                    local_mse = local_model.mse_resid
                    local_df = local_model.df_resid
                except:
                    local_mse = global_mse
                    local_df = global_df_resid
                
                pairwise_res = pairwise_lsd_test_with_mse(stats, local_mse, local_df, alpha=0.05)
                letters = solve_clique_cld(stats['mean'], pairwise_res, use_uppercase=False)
            
            for lvl in stats.index:
                lvl_str = str(lvl).strip()
                mean_val = stats.loc[lvl, 'mean']
                let = letters.get(lvl_str, 'a')
                
                row = current_info.copy()
                row[test_factor] = lvl_str
                row['Mean'] = mean_val
                row['SD'] = stats.loc[lvl, 'std']
                row['Letter'] = let
                row['Mean_Letter'] = f"{mean_val:.2f} {let}"
                
                res['sliced_comparison_rows'].append(row)
                
    except Exception as e:
        res['error'] = f"æŒ‡æ ‡ '{target}' å‡ºé”™: {str(e)}"
    
    return res

# ==========================================
# 3. åç«¯é€»è¾‘ (Cached + Multiprocessing)
# ==========================================

# ä½¿ç”¨ @st.cache_data ç¼“å­˜è®¡ç®—ç»“æœï¼Œé¿å…åˆ·æ–° UI æ—¶é‡æ–°è·‘è®¡ç®—
@st.cache_data(show_spinner=False) 
def compute_all_stats(df, factors, valid_targets, test_factor, mse_strategy):
    """
    åç«¯æ ¸å¿ƒè®¡ç®—å‡½æ•°ï¼šè´Ÿè´£è°ƒåº¦è¿›ç¨‹æ± æˆ–çº¿ç¨‹æ± ã€‚
    """
    
    # å‡†å¤‡å·¥ä½œ
    work_df = df.copy()
    for f in factors:
        work_df[f] = work_df[f].astype(str).str.strip()
    
    # 1. ç­–ç•¥é€‰æ‹©ï¼šæ ¹æ®æ•°æ®é‡å†³å®šå¹¶å‘æ¨¡å‹
    # ç»Ÿè®¡æ¨¡å‹è®¡ç®—éå¸¸è€— CPUï¼Œå¤šè¿›ç¨‹(Process)èƒ½ç»•è¿‡ GILï¼Œä½†å¯åŠ¨æœ‰å¼€é”€ã€‚
    # é˜ˆå€¼ï¼šå¦‚æœä»»åŠ¡å°‘äº 5 ä¸ªï¼Œç›´æ¥ä¸²è¡Œåè€Œæ›´å¿«ã€‚
    num_tasks = len(valid_targets)
    use_multiprocessing = num_tasks > 5
    
    # ç¡®å®šæ ¸å¿ƒæ•°ï¼Œä¿ç•™ 1-2 ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿå’Œ UI
    max_cpu = os.cpu_count() or 4
    if max_cpu > 4:
        workers = max_cpu - 1
    else:
        workers = max_cpu

    results_list = []
    errors = []

    # å‡†å¤‡ä»»åŠ¡å‚æ•°åˆ—è¡¨
    tasks = []
    for t in valid_targets:
        # åªä¼ é€’å¿…è¦çš„åˆ—ï¼Œå‡å°‘è¿›ç¨‹é—´é€šä¿¡å¼€é”€ (Pickle overhead)
        subset_df = work_df[[t] + factors]
        tasks.append((t, subset_df, factors, test_factor, mse_strategy))

    start_time = time.time()
    
    if use_multiprocessing:
        # ğŸš€ å¤šè¿›ç¨‹æ¨¡å¼ (ProcessPoolExecutor) - çœŸæ­£å¹¶è¡Œ
        # æ³¨æ„ï¼šåœ¨ Streamlit ä¸­ï¼ŒProcessPoolExecutor å¿…é¡»å°å¿ƒä½¿ç”¨ï¼Œç¡®ä¿å‡½æ•°åœ¨é¡¶å±‚
        with concurrent.futures.ProcessPoolExecutor(max_workers=workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = [executor.submit(process_single_target, *task) for task in tasks]
            # è·å–ç»“æœ (as_completed å…è®¸æˆ‘ä»¬ç›‘æ§è¿›åº¦ï¼Œä½†ä¸ºäº†ç¼“å­˜æ–¹ä¾¿ï¼Œè¿™é‡Œç›´æ¥ map ä¹Ÿå¯ä»¥)
            # ä¸ºäº†èƒ½åœ¨å¤–éƒ¨æ›´æ–°è¿›åº¦æ¡ï¼Œæˆ‘ä»¬éœ€è¦ yield æˆ–è€…è¿”å› futuresï¼Œä½†åœ¨ cache å‡½æ•°é‡Œè¿™å¾ˆå¤æ‚
            # è¿™é‡Œæˆ‘ä»¬ä¸ºäº†é€Ÿåº¦ï¼Œç›´æ¥ç­‰å¾…æ‰€æœ‰ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                try:
                    res = future.result()
                    results_list.append(res)
                except Exception as e:
                    errors.append(f"System Error: {e}")
    else:
        # ğŸ¢ å°‘é‡ä»»åŠ¡ç›´æ¥ä¸²è¡Œ/çº¿ç¨‹æ±  (å¼€é”€å°)
        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            futures = [executor.submit(process_single_target, *task) for task in tasks]
            for future in concurrent.futures.as_completed(futures):
                 results_list.append(future.result())

    elapsed = time.time() - start_time
    
    return results_list, errors, elapsed

def process_results_to_dfs(results_list, factors, test_factor, valid_targets, work_df):
    """
    å°†è®¡ç®—ç»“æœåˆ—è¡¨è½¬æ¢ä¸º DataFrameï¼Œé€Ÿåº¦æå¿«ï¼Œæ— éœ€å¹¶è¡Œã€‚
    """
    all_anova = []
    all_main = []
    all_sliced = []
    errors = []

    for res in results_list:
        if res.get('error'):
            errors.append(res['error'])
        else:
            all_anova.extend(res['anova_rows'])
            all_main.extend(res['main_effects_rows'])
            all_sliced.extend(res['sliced_comparison_rows'])

    final_res = {'errors': errors}

    # è¡¨æ ¼ 1: ANOVA
    if all_anova:
        final_res['anova_table'] = pd.DataFrame(all_anova).pivot_table(
            index='Source', columns='Trait', values='F_Sig', aggfunc='first'
        )
    else:
        final_res['anova_table'] = pd.DataFrame()

    # è¡¨æ ¼ 2: Main Effects
    if all_main:
        me_df = pd.DataFrame(all_main)
        me_pivot = me_df.pivot_table(
            index=['Factor', 'Level'], columns='Trait', values=['Mean_Letter'], aggfunc='first'
        )
        final_res['main_effects_table'] = me_pivot.swaplevel(0, 1, axis=1).sort_index(axis=1)
    else:
        final_res['main_effects_table'] = pd.DataFrame()

    # è¡¨æ ¼ 3: Sliced
    if all_sliced:
        sc_df = pd.DataFrame(all_sliced)
        group_factors = [f for f in factors if f != test_factor]
        pivot_index = group_factors + [test_factor]
        
        # åˆ†åˆ—æ•°æ®
        sc_pivot_sep = sc_df.pivot_table(
            index=pivot_index, columns='Trait', values=['Mean', 'Letter', 'SD'], aggfunc='first'
        )
        sc_pivot_sep = sc_pivot_sep.swaplevel(0, 1, axis=1).sort_index(axis=1, level=0)
        
        # æ’åºç¾åŒ–
        sorted_traits = sc_pivot_sep.columns.get_level_values(0).unique()
        new_columns = []
        for t in sorted_traits:
            for val in ['Mean', 'Letter', 'SD']:
                if (t, val) in sc_pivot_sep.columns:
                    new_columns.append((t, val))
        final_res['sliced_table_sep'] = sc_pivot_sep.reindex(columns=new_columns)
        
        # ç»„åˆæ•°æ®
        sc_pivot_comb = sc_df.pivot_table(
            index=pivot_index, columns='Trait', values=['Mean_Letter'], aggfunc='first'
        )
        final_res['sliced_table_comb'] = sc_pivot_comb.swaplevel(0, 1, axis=1).sort_index(axis=1)
    else:
        final_res['sliced_table_sep'] = pd.DataFrame()
        final_res['sliced_table_comb'] = pd.DataFrame()

    # è¡¨æ ¼ 4: Correlation (NumPy è®¡ç®—æå¿«ï¼Œç›´æ¥åœ¨è¿™é‡Œåš)
    if len(valid_targets) > 1:
        # ç¡®ä¿ç±»å‹æ­£ç¡®
        num_df = work_df[valid_targets].apply(pd.to_numeric, errors='coerce')
        corr_df = num_df.corr() 
        pval_df = num_df.corr(method=lambda x, y: pearsonr(x, y)[1]) 
        
        corr_matrix = pd.DataFrame(index=valid_targets, columns=valid_targets)
        for r_idx in valid_targets:
            for c_idx in valid_targets:
                if r_idx == c_idx:
                    corr_matrix.loc[r_idx, c_idx] = "-"
                else:
                    r = corr_df.loc[r_idx, c_idx]
                    p = pval_df.loc[r_idx, c_idx]
                    if pd.isna(r):
                        corr_matrix.loc[r_idx, c_idx] = "NaN"
                    else:
                        corr_matrix.loc[r_idx, c_idx] = f"{r:.2f}{get_stars(p)}"
        final_res['correlation'] = corr_matrix
    else:
        final_res['correlation'] = pd.DataFrame()
        
    return final_res

# ==========================================
# 4. Streamlit ç•Œé¢
# ==========================================

st.set_page_config(page_title="æé€Ÿæ•°æ®åˆ†æ", layout="wide", page_icon="âš¡")
st.title("âš¡ æé€Ÿç»Ÿè®¡åˆ†æ (Pro)")

# ä¾§è¾¹æ 
with st.sidebar:
    styled_tag("æ•°æ®ä¸Šä¼ ", icon="ğŸ“‚")
    uploaded_file = st.file_uploader("é€‰æ‹© Excel/CSV æ–‡ä»¶", type=['xlsx', 'csv'])
    
    styled_tag("å› å­é€‰æ‹©", icon="ğŸ§¬")
    
    factors = []
    targets = []
    test_factor = None
    mse_strategy = 'oneway' 
    
    df = None
    
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                excel_file = pd.ExcelFile(uploaded_file)
                sheet_names = excel_file.sheet_names
                if len(sheet_names) > 1:
                    st.success(f"ğŸ“‚ åŒ…å« {len(sheet_names)} ä¸ªSheet")
                    selected_sheet = st.selectbox("é€‰æ‹©å·¥ä½œè¡¨:", sheet_names)
                    df = excel_file.parse(selected_sheet)
                else:
                    df = excel_file.parse(0)
            
            df.columns = df.columns.astype(str)
            all_cols = df.columns.tolist()
            
            st.markdown("---")
            factors = st.multiselect("å› å­ (X)", all_cols)
            
            if factors:
                default_idx = len(factors) - 1
                test_factor = st.selectbox("æ¯”è¾ƒå› å­ (ç”¨äºç»„å†…æ¯”è¾ƒ)", factors, index=default_idx)
            
            targets = st.multiselect("æŒ‡æ ‡ (Y)", all_cols)
            
            st.markdown("---")
            with st.expander("âš™ï¸ æ¨¡å‹è®¾ç½® (é»˜è®¤å•å› ç´ )", expanded=False):
                strategy_label = st.radio(
                    "è¯¯å·®è®¡ç®—æ–¹å¼ (ä¸»æ•ˆåº”)",
                    ('å¤šå› ç´ æ¨¡å‹è¯¯å·®(GLM)', 'å•å› ç´ æ¨¡å‹è¯¯å·®'),
                    index=1
                )
                mse_strategy = 'full' if 'å¤šå› ç´ ' in strategy_label else 'oneway'
            
        except Exception as e:
            st.error(f"è¯»å–é”™è¯¯: {e}")

# ä¸»ç•Œé¢åŒºåŸŸ
if not (uploaded_file and factors and targets and test_factor):
    with st.expander("â„¹ï¸ ä½¿ç”¨è¯´æ˜(ç‚¹å‡»å±•å¼€)", expanded=True):
        st.markdown("""
        ### ğŸš€ ä¼˜åŒ–è¯´æ˜
        æ­¤ç‰ˆæœ¬å¯ç”¨äº†**å¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—**å’Œ**æ™ºèƒ½ç¼“å­˜**ï¼š
        1. **ä¸å¡é¡¿**ï¼šè®¡ç®—é€»è¾‘ä¸ç•Œé¢åˆ†ç¦»ï¼Œé¿å…è¿›åº¦æ¡é¢‘ç¹åˆ·æ–°å¯¼è‡´çš„å¡æ­»ã€‚
        2. **æ›´å¿«é€Ÿ**ï¼šé’ˆå¯¹ CPU å¯†é›†å‹ä»»åŠ¡ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºå¤šæ ¸å¹¶è¡Œå¤„ç†ï¼ˆç»•è¿‡ GIL é”ï¼‰ã€‚
        3. **ç§’åˆ‡æ¢**ï¼šåˆ†æå®Œæˆåï¼Œåˆ‡æ¢ Tab æˆ–ä¿®æ”¹å±•ç¤ºé€‰é¡¹æ—¶**æ— éœ€é‡æ–°è®¡ç®—**ã€‚
        """)
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ æ•°æ®å¹¶é…ç½®å‚æ•°")
else:
    st.markdown("###") 
    c1, c2, c3 = st.columns([1, 2, 1])
    
    # ä½¿ç”¨ Session State æ¥è®°å½•æŒ‰é’®ç‚¹å‡»çŠ¶æ€ï¼Œé˜²æ­¢é¡µé¢åˆ·æ–°ä¸¢å¤±
    if 'run_analysis' not in st.session_state:
        st.session_state.run_analysis = False

    with c2:
        if st.button("ğŸš€ å¯åŠ¨å¹¶è¡Œåˆ†æ", type="primary", use_container_width=True):
            st.session_state.run_analysis = True

    if st.session_state.run_analysis:
        st.divider()
        
        # 1. é¢„å¤„ç†æ•°æ® (è½»é‡)
        valid_targets = []
        for t_col in targets:
            # ç®€å•æ£€æŸ¥ï¼Œä¸æ¶‰åŠé‡è®¡ç®—
            if pd.to_numeric(df[t_col], errors='coerce').notna().sum() > 0:
                valid_targets.append(t_col)
        
        if not valid_targets:
            st.error("æ‰€é€‰æŒ‡æ ‡å‡ä¸ºç©ºæˆ–éæ•°å€¼ï¼")
            st.stop()

        # 2. è°ƒç”¨æ ¸å¿ƒè®¡ç®— (å¸¦ç¼“å­˜ + å¤šè¿›ç¨‹)
        with st.spinner(f"æ­£åœ¨å…¨é€Ÿè®¡ç®— {len(valid_targets)} ä¸ªæŒ‡æ ‡ï¼Œè¯·ç¨å€™..."):
            # ä¼ é€’ df çš„å‰¯æœ¬ä»¥é˜²ä¿®æ”¹
            # æ³¨æ„ï¼šStreamlit ç¼“å­˜æ˜¯åŸºäºå‚æ•°å“ˆå¸Œçš„ï¼Œæ‰€ä»¥å‚æ•°æ²¡å˜å°±ä¸ä¼šé‡è·‘
            raw_results, exec_errors, elapsed_time = compute_all_stats(
                df, factors, valid_targets, test_factor, mse_strategy
            )

        if exec_errors:
            with st.expander("âš ï¸ è®¡ç®—è¿‡ç¨‹ä¸­çš„è­¦å‘Š", expanded=False):
                for err in exec_errors:
                    st.warning(err)

        # 3. æ•´ç†ç»“æœ (æå¿«)
        final_res = process_results_to_dfs(raw_results, factors, test_factor, valid_targets, df)
        
        st.success(f"âœ… åˆ†æå®Œæˆï¼è€—æ—¶: {elapsed_time:.2f} ç§’ (å·²ç¼“å­˜)")

        # 4. å±•ç¤º Tab
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ“ˆ ç»„å†… (åˆ†åˆ—)", 
            "ğŸ“‘ ç»„å†… (ç»„åˆ)", 
            "ğŸ† ä¸»æ•ˆåº”", 
            "ğŸ§® ANOVA", 
            "ğŸ”— ç›¸å…³æ€§"
        ])
        
        with tab1:
            if not final_res['sliced_table_sep'].empty:
                st.dataframe(final_res['sliced_table_sep'], use_container_width=True)
            else: st.warning("æ— æ•°æ®")

        with tab2:
            if not final_res['sliced_table_comb'].empty:
                st.dataframe(final_res['sliced_table_comb'], use_container_width=True)
            else: st.warning("æ— æ•°æ®")

        with tab3:
            if not final_res['main_effects_table'].empty:
                st.dataframe(final_res['main_effects_table'], use_container_width=True)
            else: st.warning("æ— æ•°æ®")

        with tab4:
            if not final_res['anova_table'].empty:
                st.dataframe(final_res['anova_table'], use_container_width=True)
            else: st.warning("æ— æ•°æ®")

        with tab5:
            if not final_res['correlation'].empty:
                st.dataframe(final_res['correlation'], use_container_width=True)
            else: st.info("æ— ç›¸å…³æ€§æ•°æ®")
        
        # 5. ä¸‹è½½é€»è¾‘
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer) as writer:
            if not final_res['sliced_table_sep'].empty: 
                final_res['sliced_table_sep'].to_excel(writer, sheet_name='ç»„å†…_åˆ†åˆ—æ•°æ®')
            if not final_res['sliced_table_comb'].empty: 
                final_res['sliced_table_comb'].to_excel(writer, sheet_name='ç»„å†…_ç»„åˆæ ‡ç­¾')
            if not final_res['main_effects_table'].empty: 
                final_res['main_effects_table'].to_excel(writer, sheet_name='ä¸»æ•ˆåº”_å¤§å†™')
            if not final_res['anova_table'].empty: 
                final_res['anova_table'].to_excel(writer, sheet_name='ANOVA')
            if not final_res['correlation'].empty: 
                final_res['correlation'].to_excel(writer, sheet_name='ç›¸å…³åˆ†æ')
            
        st.download_button(
            "ğŸ“¥ ä¸‹è½½å®Œæ•´ç»“æœ (Excel)",
            data=buffer.getvalue(),
            file_name=f"FastAnalysis_Result.xlsx",
            mime="application/vnd.ms-excel"
        )
